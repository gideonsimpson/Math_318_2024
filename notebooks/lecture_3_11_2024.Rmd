---
title: "3/11/2024 Lecture"
output: html_notebook
---

```{r}
library(tidyverse)
library(glmnet) # this provides ridge regression
```

# Ridge Regression in R

## Generate Data

Revisit the problem with more predictors than observations AND $y$ only really depends on $x_1$

```{r}
set.seed(100)

# number of observations
n = 500
# total number of predictors
p = 600
# the x1 predictor and y
x1 <- runif(0,1,n=n)
y <- 1 + 2 * x1 + 0.1 * rnorm(n) #true model, \beta_0 = 1, \beta_1 = 2

# fill the other p-1 variables with Gaussian noise by
# first populating a matrix
x_rest <- matrix(0.01 * rnorm(n *(p-1)), n, p-1)
# then we name the columns using colnames and paste
colnames(x_rest) <- paste("x",2:p, sep="")
# now we glue the columns together into a single matrix
data.matrix <- cbind(x1, x_rest, y)

# this reinterprets the matrix as a data frame
data.df <- as_tibble(data.matrix)
```

## Apply Ridge Regression

To use `glmnet` for Ridge Regression, we need to extract the design matrix (without the intercept column), and store it as a matrix:

```{r}
X = model.matrix(y~., data.df)[,-1] # -1 omits the first column, corresponding to the intercept, this handles scaling
```

```{r}
X
```

Fitting is then very similar to `glm`; however, we need to specify the $\lambda>0$ . The standard approach is not to do this for a single $\lambda$, but instead, an array of them:

$$
\lambda = 10^{q}
$$

over a range of $q$

```{r}
low = -4;
high = 4;
lambda.grid = 10^seq(high, low,length = 51); # decreasing order
lambda.grid
```

Perform ridge regression (`alpha=0` to get ridge)

```{r}
ridge.fit = glmnet(X,y, alpha = 0, lambda = lambda.grid)
ridge.fit
```

## Evaluating Results

Get the coefficients and store in a data frame:

```{r}
beta = as.matrix(coef(ridge.fit))
beta.df = as_tibble(beta)
beta.df$coef = row.names(beta)
beta.df
```

Reformat the data

```{r}
# spread out
beta.df.long = gather(beta.df,key=case,value,-coef)
# relabel columns
beta.df.long$case = as.integer(gsub("s", "", beta.df.long$case))
beta.df.long$lambda =ridge.fit$lambda[beta.df.long$case+1]
beta.df.long
```

Visualize the coefficients as a function of $\lambda$:

```{r}
# plot all coefficients
beta.plt = ggplot(beta.df.long[beta.df.long$coef!="(Intercept)",], 
                   aes(x=lambda, y=value,color = coef,linetype = coef)) + 
  geom_line() + theme(legend.position="none") +
  scale_x_log10()  + ggtitle("Ridge Regression Coefficients") + ylab("Coefficients")
print(beta.plt)
```

As $\lambda$ increases, we drive all $\beta$ to zero.

Study a single coefficient:

```{r}
beta1.plt= ggplot(beta.df.long[beta.df.long$coef=="x1",], 
                  aes(x=lambda, y=value,
color = coef, linetype = coef)) +
geom_line() + theme(legend.position="none") +
ggtitle("beta 1 Ridge Regression Coefficient") +
scale_x_log10()
print(beta1.plt)
```

Make new predictions with our model. `s` corresponds to $\lambda$. The `t` is for transpose

```{r}
predict(ridge.fit, s=1000, newx=t(runif(p))) 
```

```{r}
mean(y)
```

In large $\lambda$ limit, we predict the sample mean, $\bar{y}$

This shows that when $\lambda$ is large, we have zeroed out all the coefficients, and our prediction is $\hat{\beta}_0$, which is just `mean(y)`

```{r}
predict(ridge.fit, s=.1, newx=t(runif(p)))
```

## Cross Validate

This is how we choose the optimal $\lambda$

```{r}
cv.ridge = cv.glmnet(X, y, alpha = 0, lambda = lambda.grid)
```

`alpha=0` - this corresponds to ridge regression

```{r}
cv.ridge
```

`s=cv.ridge$lambda.min` gets the coefficients with the MSE minimizing $\lambda$. We can visualize this as:

```{r}
# visualize using ggplot2
cv.df=tibble(lambda=cv.ridge$lambda, 
            cvm=cv.ridge$cvm, 
            cvlo=cv.ridge$cvlo,
            cvup=cv.ridge$cvup)

cv.plt = ggplot(cv.df) + geom_point(aes(x=lambda,y=cvm),color="red") +
geom_errorbar(aes(x=lambda, ymin=cvlo, ymax=cvup),alpha = .25) +
geom_vline(xintercept = cv.ridge$lambda.min,linetype = "dashed")+
geom_vline(xintercept = cv.ridge$lambda.1se,linetype = "dashed")+
ggtitle("Ridge Regression Error") +
ylab("Cross Validated MSE") +
scale_x_log10()
print(cv.plt)
```

Extract the coefficients at the estimated optimal choice of $\lambda$:

```{r}
beta.minimal = predict(ridge.fit, type = "coefficients", s = cv.ridge$lambda.min)
beta.minimal[1:10]
```

# LASSO

## Apply LASSO

`alpha=1` for LASSO.

```{r}
lasso.fit = glmnet(X,y, alpha = 1, lambda = lambda.grid)
lasso.fit
```

## Evaluate Results

```{r}
beta.lasso = as.matrix(coef(lasso.fit))
beta.lasso.df = as_tibble(beta.lasso)
beta.lasso.df$coef = row.names(beta.lasso)
beta.lasso.df
```

```{r}
# spread out
beta.lasso.df.long = gather(beta.lasso.df,key=case,value,-coef)
# relabel columns
beta.lasso.df.long$case = as.integer(gsub("s", "", beta.lasso.df.long$case))
beta.lasso.df.long$lambda =lasso.fit$lambda[beta.lasso.df.long$case+1]
beta.lasso.df.long
```

```{r}
# plot all coefficients
beta.lasso.plt = ggplot(beta.lasso.df.long[beta.lasso.df.long$coef!="(Intercept)",], 
                   aes(x=lambda, y=value,color = coef,linetype = coef)) + 
  geom_line() + theme(legend.position="none") +
  scale_x_log10()  + 
  ggtitle("LASSO Coefficients") + 
  ylab("Coefficients")
print(beta.lasso.plt)
```

LASSO does variable **selection**:

```{r}
beta.lasso.minimal = predict(lasso.fit, type = "coefficients", s = cv.lasso$lambda.min)
beta.lasso.minimal[1:10]
```

## Cross Validate

```{r}
cv.lasso = cv.glmnet(X, y, alpha = 1, lambda = lambda.grid)
```

```{r}
cv.lasso.df=tibble(lambda=cv.lasso$lambda, 
            cvm=cv.lasso$cvm, 
            cvlo=cv.lasso$cvlo,
            cvup=cv.lasso$cvup)

cv.plt = ggplot(cv.lasso.df) + geom_point(aes(x=lambda,y=cvm),color="red") +
geom_errorbar(aes(x=lambda, ymin=cvlo, ymax=cvup),alpha = .25) +
geom_vline(xintercept = cv.lasso$lambda.min,linetype = "dashed")+
geom_vline(xintercept = cv.lasso$lambda.1se,linetype = "dashed")+
ggtitle("LASSO Error") +
ylab("Cross Validated MSE") +
scale_x_log10()
print(cv.plt)
```

# Unsupervised Learning

```{r}
library(mvtnorm)
library(tidyverse)
set.seed(100)
mu <- c(3,-5)
sigma <- matrix(c(2, 1, 1, 2), nrow=2)
xy.samples <- as_tibble(rmvnorm(10^3, mean = mu, sigma = sigma))
colnames(xy.samples) <- c("x","y")
xy.plt <- ggplot(data=xy.samples,
mapping = aes(x=x,y=y)) +geom_point()
print(xy.plt)
```

```{r}

set.seed(100)

mu1 <- c(2,-4)
sigma1 <- matrix(c(2, 1, 1, 2), nrow=2)

mu2 <- c(-1,2)
sigma2 <- matrix(c(3, -0.1, -0.1, 3), nrow=2)


samples1 <- rmvnorm(10^3, mean = mu1, sigma = sigma1)
samples2 <- rmvnorm(10^3, mean = mu2, sigma = sigma2)

clusters.samples <- as_tibble(rbind(samples1, samples2))
colnames(clusters.samples) <- c("x","y")
clusters.plt <- ggplot(data=clusters.samples, 
                 mapping = aes(x=x,y=y)) +geom_point()
print(clusters.plt)
```
