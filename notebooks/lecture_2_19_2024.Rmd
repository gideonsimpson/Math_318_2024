---
title: "2/19/2024 Lecture"
output: html_notebook
editor_options: 
  markdown: 
    wrap: 72
---

```{r}
library(tidyverse)
library(ISLR2)
```

# Randomized Splits

Randomly choose entries for training/testing, to remove possible bias in
the data due to how it was entered.

Do this with the `Default` data set:

```{r}
Default = Default %>% as_tibble
```

Number of samples in the data set:

```{r}
nrow(Default)
```

Choose approximately, 70% of the entries randomly:

```{r}
n_train = floor(.7 * nrow(Default)) # floor is there to get an integer

set.seed(100) # since we are randomly sampling
train.idx = sample(nrow(Default), size=n_train) # sample 1:nrow(Default) n_train times, without replacement
```

```{r}
train.idx
```

Split into random training/testing sets:

```{r}
train.df = Default[train.idx,]
test.df = Default[-train.idx,] # the -train.idx means take the complmentary set of rows
```

Copying over the code from the last lecture:

Train the classifier

```{r}
logistic.train = glm(default~balance, data = train.df, family=binomial)
logistic.train
```

Evaluate performance:

```{r}
train.prob = predict(logistic.train, newdata = train.df, type="response")
# turn probabilities into classes
train.pred = rep("No", nrow(train.df))
train.pred[train.prob>0.5] = "Yes"
mean(as.numeric(train.pred != train.df$default))
```

```{r}
# get predicted probabiliities of Yes, will default
test.prob = predict(logistic.train, newdata = test.df, type="response")
# turn probabilities into classes
test.pred = rep("No", nrow(test.df))
test.pred[test.prob>0.5] = "Yes"
mean(as.numeric(test.pred != test.df$default))
```

This. is similar to what we had when we deterministically chose
training/testing sets.

# Logistic Classifiers with Two Predictors

Can we do better in the `Default` data set with more than predictor? Try
training against `balance` and `income`.

Look at data graphically, when plotting against both features

```{r}
logistic.plt = ggplot(Default, aes(shape=default, color=default)) +
  geom_point(aes(balance, income))
print(logistic.plt)
```

Clearly a muddled region around, say, balance = 1500, where it's
ambiguous. Really need another feature to best distinguish these two
classes. More samples is unlikely to resolve the issue.

```{r}
logistic2d.train = glm(default~balance+income, data = train.df, family=binomial)
logistic2d.train
```

Could then do testing/training comparison on this.

# Gaussian Mixture Models

## Generate Data

```{r}
set.seed(100) 
# set an array with means -2, 1 
m = c(-2, 1)
# set an array of std. dev's to 2 and 1 
sigma = c(2., 0.5) 
# set an array of prior probabilities
prior = c(0.4, 0.6) 
# set number of samples 
n = 10^4

```

```{r}
# sampling from the classes according to the prior

# This samples from {1,2}, n times, with replacement

# with P(1) = prior[1] and P(2) = prior[2]

class= sample(1:2,n,replace = TRUE, prob = prior)
# interpret as a categorical variable 
class = as.factor(class) # these are the hidden variables
# preallocates the x array
x = numeric(n) 
# populates the x array 
for(j in 1:n){
  x[j] =rnorm(1,mean = m[class[j]], sd = sigma[class[j]]) 
  } 
# store data as a data frame 
mixture.df = tibble("class"=class, "x"=x)
```

## Visualize

```{r}

# construct a histogram of just the x values
hist.all = ggplot(mixture.df, aes(x)) + geom_histogram(bins = 50) +
ggtitle("All Data") 
# show the plot 
print(hist.all)
```

```{r}
# alpha sets transparency, use position="identity" to split # the histogram by classes 
hist.split = ggplot(mixture.df, aes(x=x, fill=class)) + 
  geom_histogram(alpha=0.5, bins=50, position ="identity") + 
  ggtitle("Data Split by Class") + labs(fill="Class")
#show the plot 
print(hist.split)
```
