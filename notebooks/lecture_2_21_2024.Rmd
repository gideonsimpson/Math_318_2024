---
title: "2/21/2024 Lecture"
output: html_notebook
---

```{r}
library(tidyverse)
library(ISLR2)
library(MASS) # for LDA and QDA regressions
```

# Bayesian Classifiers

## LDA in 1 Variable

### Generate mixture model data

```{r}
set.seed(100) # set seed for reproducibility
# defines and the standard devs of the two Gaussians
mu=c(-1, 2)
sigma=c(1., 1.)
# prior probabilities
prior=c(0.3, 0.7)

# number of total samples we draw from the mixture model
n = 10^4

# first sample the class varaible n times
class=sample(1:2,n,replace = TRUE, prob = prior)
# convert to categorical variable
class=as.factor(class)
# sample the x values (what we measure in practice)
# preallocates the array
x=numeric(n)
for(j in 1:n){
  # sample Gaussian 1 or Gaussian 2 as appropriate
  x[j] =rnorm(1,mean = mu[class[j]], sd = sigma[class[j]])
}
# store as a tibble
mixture.df <- tibble("class"=class, "x"=x)
mixture.df
```

```{r}
hist.split = ggplot(mixture.df, aes(x=x, fill=class)) + 
  geom_histogram(alpha=0.5, bins=50, position ="identity") + 
  ggtitle("Data Split by Class") + labs(fill="Class")
#show the plot 
print(hist.split)
```

Note the overlap region; this is where the errors will be.

### the LDA Model

```{r}
lda.fit = lda(class~x, data = mixture.df) # fits the LDA classifier, given the data, and the model class ~ x
lda.fit
```

### Check Performance

```{r}
# generates the predictions on the trained classifier
pred.train = predict(lda.fit,newdata = mixture.df)
# construct a data frame for plotting
lda.df=tibble(x=mixture.df$x, true = mixture.df$class, 
              pred=pred.train$class, 
              p=pred.train$posterior[,1], # predicted and probability
              correct = (mixture.df$class==pred.train$class)) # correct or not

```

Accuracy

```{r}
mean(lda.df$correct)
```

Visualize

```{r}
plot.lda = ggplot(data = lda.df, aes(shape=pred, color=correct)) + 
  geom_point(aes(x=x,y=p)) + 
  ggtitle("LDA Classifier")
print(plot.lda)
```

Note: The errors all occur in the region where the Gaussians overlap.

### Compare with Logistic

```{r}
logistic.fit=glm(class~x,data=mixture.df, family = binomial)
logistic.prob=as_tibble(predict(logistic.fit, type = "response"))
logistic.pred=rep(1, nrow(mixture.df))
logistic.pred[logistic.prob>0.5] <-2
logistic.df=tibble(x=mixture.df$x, true = mixture.df$class,
                   pred=as.factor(logistic.pred), 
                   p=pred.train$posterior[,1],
                   correct = (mixture.df$class==logistic.pred))
```

```{r}
mean(logistic.df$correct)
```

Not so different than LDA.

```{r}
plot.logistic= ggplot(data = logistic.df, aes(shape=pred, color=correct)) +
  geom_point(aes(x=x,y=p)) + 
  ggtitle("Logistic Classifier")
print(plot.logistic)

```

## LDA in 2 Variables with More Classes

### Generate mixture model data

Create a Gaussian mixture model of three classes in 2D

```{r}
set.seed(100)

# true means
mu1=c(-1, -1)
mu2=c(-1, 1)
mu3=c(2,0)
# true covariances
sigma1vals=c(1., 1.)
sigma2vals=c(1., 1.)
sigma3vals=c(1., 2.)
# priors
prior=c(0.25, 0.5, 0.25)
# total number of samples
n = 10^4

# training data
# sample with replacement from 1,2,3 n times
# with prob(1) = pi[1], prob(2)=pi[2], prob(3)=pi[3]
class=sample(1:3, n, replace = TRUE, prob=prior)
# interpret as a categorical variable
class=factor(class)
x=numeric(n)
y=numeric(n)
for(j in 1:n){
  # loop through switching between cases
  if(class[j]==1){
    x[j] = rnorm(1,mean = mu1[1], sd = sigma1vals[1])
    y[j] = rnorm(1,mean = mu1[2], sd = sigma1vals[2])
  }else if(class[j]==2){
    x[j] = rnorm(1,mean = mu2[1], sd = sigma2vals[1])
    y[j] = rnorm(1,mean = mu2[2], sd = sigma2vals[2])
  }else{
    x[j] = rnorm(1,mean = mu3[1], sd = sigma3vals[1])
    y[j] = rnorm(1,mean = mu3[2], sd = sigma3vals[2])
  }
}
train.df <- tibble("class"=class, "x"=x, "y"=y)

```

```{r}
mixture2d.plt=ggplot(train.df, aes(color=class)) + 
  geom_point(aes(x=x,y=y))
print(mixture2d.plt)

```

```{r}
train.df
```

### Train the LDA

```{r}
lda.2d = lda(class~x+y, data = train.df)
print(lda.2d)
```

### Check Performance

```{r}
pred.train=predict(lda.2d)
mean(pred.train$class != train.df$class)
table(pred.train$class, train.df$class)

```
