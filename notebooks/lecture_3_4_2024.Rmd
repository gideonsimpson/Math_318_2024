---
title: "3/4/2024 Lecture"
output: html_notebook
---

```{r}
library(tidyverse)
library(MASS)
library(FNN)
library(ISLR2)
```

# Cross Validation

## LOOCV

`cv.glm` is part of part of `boot`, and it does the cross validation

```{r}
library(boot)
```

After setting up `glm.fit`, run `cv.err = cv.glm(train.df,glm.fit)` to do cross validation.

### Create Data

```{r}
set.seed(100)

# test/train sample size
n = 10^2
# generate the training set
x = runif(n,0,5)
y = exp(x)*(1 + .5 * rnorm(n))
train.df = tibble("x"=x, "y"=y)

```

### Loop over Models

Try different polynomial fits

```{r}

maxp = 10

# preallocate an array with zeros
loocv.MSE.vals = rep(0,maxp)


# loop over data sets and polynomial fits
for(p in seq(maxp)){
    glm.fit = glm(y~poly(x,p), data=train.df)
    # run LOOCV cross validation
    cv.err = cv.glm(train.df,glm.fit)
    # actual errors are stored in $delta[1]
    loocv.MSE.vals[p] =cv.err$delta[1]
}

loocv.MSE.df = tibble(p = seq(maxp), MSE=loocv.MSE.vals)

```

Visualize:

```{r}

loocv.plt = ggplot(loocv.MSE.df,mapping = aes(x=p, y=MSE)) +
  geom_line(lwd=2) +
  labs(x ="Degree Polynomial", y = "MSE", title="LOOCV MSE as a Function of Fit") 
print(loocv.plt)
```

Simplest model within this family of polynomials is degree 3

## k-Fold Cross Validation (CV)

Typically do k-Fold instead, with $k=5, 10$. $k$-Fold is done with `cv.glm` with a `K=` argument for the number of folds. If no argument is given, defaults to LOOCV.

This script compares the error across LOOCV, 5-fold and 10-fold.

```{r}
# create arrays
MSE.vals = NULL
Method.vals = NULL
p.vals = NULL

# loop over data sets and polynomial fits
for(p in seq(maxp)){
  glm.fit = glm(y~poly(x,p), data=train.df)
  
  # defaults to LOOCV 
  cv.err = cv.glm(train.df,glm.fit)
  p.vals = append(p.vals,p)
  MSE.vals = append(MSE.vals,cv.err$delta[1])
  Method.vals = append(Method.vals,"LOOCV")

  # 5-fold CV
  cv.err = cv.glm(train.df,glm.fit, K=5)
  p.vals = append(p.vals,p)
  MSE.vals = append(MSE.vals,cv.err$delta[1])
  Method.vals = append(Method.vals, "5-Fold CV")

  # 10-fold CV
  cv.err = cv.glm(train.df,glm.fit, K=10)
  p.vals = append(p.vals,p)
  MSE.vals = append(MSE.vals,cv.err$delta[1])
  Method.vals = append(Method.vals, "10-Fold CV")
  
}
# store results of experiment in a data frame
MSE.df = tibble(p=p.vals, MSE=MSE.vals, Method=Method.vals)
MSE.df$Method = as.factor(MSE.df$Method)
#plot
cv.plt = ggplot(MSE.df,mapping = aes(x=p, y=MSE, color=Method)) +
  geom_line(lwd=2) + 
  labs(x ="Degree Polynomial", y = "MSE", title="MSE as a Function of Fit and CV Method")

print(cv.plt)
```

$k$-fold CV splits the data sets randomly. 10-fold is a good estimate here; we would predict to use the degree 4 polynomial.

## Classifiers

Here, we consider, in place of the MSE, the misclassification error rate. But the same techniques apply. This compares, LDA, QDA, logistic, and KNN, directly

### Create Data

```{r}
set.seed(100)

n = 10^3
#generate data
y1 <- runif(n,-3,3)
y2 <- runif(n,-3,3)
# jitter the positions
x1 <- y1+.1*rnorm(n)
x2 <- y2+.1*rnorm(n)
class <- factor(1-as.integer(0< y2-y2^3-y1))
function.df <- tibble("x1"=x1, "x2"=x2, "class"=class)
scatter.plt <- ggplot(function.df, aes(x=x1, y=x2, color=class)) +
  geom_point() 
print(scatter.plt)
```

### Create Folds

We need to manually create the folds to work with these fits. This is because we do not neccessarily directly use `glm.fit` for some of these classifier methods. This requires `caret` :

```{r}
library(caret)

```

```{r}
# set folds for Cross Validation
K = 10
folds = createFolds(seq(nrow(function.df)),k=K)
folds
```

`folds$Foldk` are indices in the fold `k` .

### Compute CV Error

We now manually loop through each of the folds, training and then testing, on all of the methods:

```{r}
logistic_err = 0
LDA_err = 0
QDA_err= 0
knn_err = 0

# loop through the folds and compute the k fold error estimates
for(j in seq(K)){
  # extracts training and testing sets
  train.df = function.df[-folds[[j]],]
  test.df = function.df[folds[[j]],]

  # train, predict, compute testing error in each case
  logistic.train = glm(class~x1+x2 , train.df, family = binomial)
  logistic.prob = predict(logistic.train,newdata = test.df, type = "response")
  logistic.pred = rep(0, nrow(test.df))
  logistic.pred[logistic.prob>0.5] = 1
  # logistic error 
  logistic_err =  logistic_err + mean(logistic.pred!= test.df$class)/K

  lda.train = lda(class~x1+x2, data = train.df)
  lda.pred = predict(lda.train, test.df)
  # LDA error
  LDA_err = LDA_err + mean(lda.pred$class !=test.df$class)/K

  qda.train = qda(class~x1+x2, data = train.df)
  qda.pred = predict(qda.train, test.df)
  # QDA error
  QDA_err = QDA_err + mean(qda.pred$class !=test.df$class)/K

  knn.pred = knn(as.matrix(train.df[1:2]), as.matrix(test.df[1:2]),
                    as.matrix(train.df[3]), k=3)
  # kNN error
  knn_err = knn_err + mean(knn.pred!= test.df$class)/K

}

```

Compare the results:

```{r}
sprintf("Logistic %d-fold Error = %g", K, logistic_err)
sprintf("LDA %d-fold Error = %g", K, LDA_err)
sprintf("QDA %d-fold Error = %g", K, QDA_err)
sprintf("KNN with k=3 %d-fold Error = %g", K, knn_err)

```

Gives us a good justification for using kNN.

# Bootstrap

Application to estimating the kurtosis of a random variable:

$$
\text{Kurtosis} = \frac{\mathbb{E}[(X-\mu)^4]}{\mathbb{E}[(X-\mu)^2]^2}
$$

This measures how far from normal a distribution is. A naive estimate is:

$$
\frac{\frac{1}{n}\sum_{i=1}^n (x_i-\hat\mu)^4}{(\frac{1}{n}\sum_{i=1}^n (x_i-\hat\mu)^2)^2}
$$

An error bound on our estimate of this can be formed by bootstrap methods.

## Generate data

First, we generate data for this experiment

```{r}
set.seed(100)

# number of samples in the data set
n <- 10^3
x <- rexp(n) # these are exponential random variables (non-Gaussians)

x.df <- tibble(x=x)
```

True kurtosis of this problem is 9

## Define Function for Bootstrapping

```{r}
kurtosis.fn = function(data,index){
  # extract values of interest in the current bootstrap sample determined by index
  x = data$x[index]
  # estimate the mean
  mu = mean(x)
  # compute the estiamted kurtosis
  k = mean((x-mu)^4)/(mean((x-mu)^2)^2)
  return (k)
}
```

## Compute Bootstrap

```{r}
bt = boot(x.df,kurtosis.fn,R=10^3) # R is the number of Bootstrap samples

```

```{r}
bt
```

```{r}
boot.ci(bt)
```

4 different types of CIs for the estimated value. True answer, 9, is in all of them.
